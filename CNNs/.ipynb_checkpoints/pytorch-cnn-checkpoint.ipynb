{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pytorch Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper parameters\n",
    "num_epochs = 5\n",
    "num_classes = 10\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "training_acc_arr = []\n",
    "testing_acc_arr = []\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data/',\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data/',\n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(7*7*32, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data Size: 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_250 = torchvision.datasets.MNIST(root='./data/',\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "train_dataset_250.targets = train_dataset.targets[:250]\n",
    "train_dataset_250.data = train_dataset.data[:250]\n",
    "\n",
    "train_loader_250 = torch.utils.data.DataLoader(dataset=train_dataset_250,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "model_250 = ConvNet(num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_250.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [5/5], Loss: 1.7185\n",
      "Epoch [2/5], Step [5/5], Loss: 1.0628\n",
      "Epoch [3/5], Step [5/5], Loss: 0.5697\n",
      "Epoch [4/5], Step [5/5], Loss: 0.4777\n",
      "Epoch [5/5], Step [5/5], Loss: 0.2932\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader_250)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader_250):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model_250(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % total_step/batch_size == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 67.59 %\n",
      "[85.2]\n",
      "[67.59]\n"
     ]
    }
   ],
   "source": [
    "model_250.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader_250:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model_250(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "training_acc_arr.append(100 * correct / total)\n",
    "\n",
    "model_250.eval() \n",
    "with torch.no_grad():  \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model_250(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "testing_acc_arr.append(100 * correct / total)\n",
    "\n",
    "print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "print(training_acc_arr)\n",
    "print(testing_acc_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data Size: 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_1000 = torchvision.datasets.MNIST(root='./data/',\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "train_dataset_1000.targets = train_dataset.targets[:1000]\n",
    "train_dataset_1000.data = train_dataset.data[:1000]\n",
    "\n",
    "train_loader_1000 = torch.utils.data.DataLoader(dataset=train_dataset_1000,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "model_1000 = ConvNet(num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_1000.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [20/20], Loss: 0.6114\n",
      "Epoch [2/5], Step [20/20], Loss: 0.4553\n",
      "Epoch [3/5], Step [20/20], Loss: 0.1959\n",
      "Epoch [4/5], Step [20/20], Loss: 0.1815\n",
      "Epoch [5/5], Step [20/20], Loss: 0.1294\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader_1000)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader_1000):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model_1000(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % total_step/batch_size == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 93.57 %\n",
      "[85.2, 98.6]\n",
      "[67.59, 93.57]\n"
     ]
    }
   ],
   "source": [
    "model_1000.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader_1000:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model_1000(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "training_acc_arr.append(100 * correct / total)\n",
    "\n",
    "model_1000.eval() \n",
    "with torch.no_grad():  \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model_1000(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "testing_acc_arr.append(100 * correct / total)\n",
    "\n",
    "print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "print(training_acc_arr)\n",
    "print(testing_acc_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data Size: 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_5000 = torchvision.datasets.MNIST(root='./data/',\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "train_dataset_5000.targets = train_dataset.targets[:5000]\n",
    "train_dataset_5000.data = train_dataset.data[:5000]\n",
    "\n",
    "train_loader_5000 = torch.utils.data.DataLoader(dataset=train_dataset_5000,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "model_5000 = ConvNet(num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_5000.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/100], Loss: 0.2590\n",
      "Epoch [2/5], Step [100/100], Loss: 0.0867\n",
      "Epoch [3/5], Step [100/100], Loss: 0.2075\n",
      "Epoch [4/5], Step [100/100], Loss: 0.0604\n",
      "Epoch [5/5], Step [100/100], Loss: 0.0312\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader_5000)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader_5000):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model_5000(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % total_step/batch_size == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 97.3 %\n",
      "[85.2, 98.6, 99.2]\n",
      "[67.59, 93.57, 97.3]\n"
     ]
    }
   ],
   "source": [
    "model_5000.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader_5000:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model_5000(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "training_acc_arr.append(100 * correct / total)\n",
    "    \n",
    "model_5000.eval() \n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model_5000(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "testing_acc_arr.append(100 * correct / total)\n",
    "print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "print(training_acc_arr)\n",
    "print(testing_acc_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data Size: 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_10000 = torchvision.datasets.MNIST(root='./data/',\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "train_dataset_10000.targets = train_dataset.targets[:10000]\n",
    "train_dataset_10000.data = train_dataset.data[:10000]\n",
    "\n",
    "train_loader_10000 = torch.utils.data.DataLoader(dataset=train_dataset_10000,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "model_10000 = ConvNet(num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_10000.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [200/200], Loss: 0.0448\n",
      "Epoch [2/5], Step [200/200], Loss: 0.1097\n",
      "Epoch [3/5], Step [200/200], Loss: 0.0247\n",
      "Epoch [4/5], Step [200/200], Loss: 0.0484\n",
      "Epoch [5/5], Step [200/200], Loss: 0.0070\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader_10000)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader_10000):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model_10000(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % total_step/batch_size == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 97.32 %\n",
      "[85.2, 98.6, 99.2, 98.81]\n",
      "[67.59, 93.57, 97.3, 97.32]\n"
     ]
    }
   ],
   "source": [
    "model_10000.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader_10000:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model_10000(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "training_acc_arr.append(100 * correct / total)\n",
    "\n",
    "model_10000.eval() \n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model_10000(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "testing_acc_arr.append(100 * correct / total)\n",
    "\n",
    "print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "print(training_acc_arr)\n",
    "print(testing_acc_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data Size: 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_30000 = torchvision.datasets.MNIST(root='./data/',\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "train_dataset_30000.targets = train_dataset.targets[:30000]\n",
    "train_dataset_30000.data = train_dataset.data[:30000]\n",
    "\n",
    "train_loader_30000 = torch.utils.data.DataLoader(dataset=train_dataset_30000 ,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "model_30000 = ConvNet(num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_30000.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [600/600], Loss: 0.0517\n",
      "Epoch [2/5], Step [600/600], Loss: 0.0277\n",
      "Epoch [3/5], Step [600/600], Loss: 0.0223\n",
      "Epoch [4/5], Step [600/600], Loss: 0.0037\n",
      "Epoch [5/5], Step [600/600], Loss: 0.0007\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader_30000)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader_30000):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model_30000(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % total_step/batch_size == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 98.54 %\n",
      "[85.2, 98.6, 99.2, 98.81, 99.28666666666666]\n",
      "[67.59, 93.57, 97.3, 97.32, 98.54]\n"
     ]
    }
   ],
   "source": [
    "model_30000.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader_30000:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model_30000(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "training_acc_arr.append(100 * correct / total)\n",
    "model_30000.eval() \n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model_30000(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "testing_acc_arr.append(100 * correct / total)\n",
    "print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "print(training_acc_arr)\n",
    "print(testing_acc_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAIABJREFUeJzt3XeYU2X2wPHvoUgRpIMIIthFVIRREVkbxQqWXUV+FhYLi2tldRWliA2xK4q6qChYQBSwi4KyK6urMJQBxAIovSNNZigD5/fH+4YJQ2YmM5PkJpnzeZ77JLn3Jvckmbknb7nvK6qKMcYYk1+5oAMwxhiTnCxBGGOMicgShDHGmIgsQRhjjInIEoQxxpiILEEYY4yJyBKE2YeIlBeRP0SkSSz3jaWgjptMRGSRiJwUdBwmfVmCSAP+RBladotITtjjK4v7eqq6S1WrqeqSWO5bXCJSS0ReF5FVIrJZRH4WkX/G+7il4WMMffa7RGRb2OO7SvG674lIn/B1qtpUVaeVPuoCj3mhiKiI3BSvY5jkViHoAEzpqWq10H0RWQRcr6qTCtpfRCqoam4iYiulIUB54GhgM3AUcEygERVBVY8K3ReR/wKvqOrrwUVUKt2B34FrgKGJPHAK/Y2mNStBlAEi8pCIvCMio0RkC3CViJwqIt+JyEYRWSkiQ0Skot+/gv/l2NQ/ftNv/0xEtojI/0SkWXH39dvPE5FfRGSTiDwnIt+IyF8LCP0k4G1V3aiqu1X1R1Udl/+4ItIkXykqW0Ryw455vYj8JCIbfFwHF/A5TRSRXvnWzRWRLiJSzr+vNT722SLSvITfx999SeN3EflYRA4Ke08vishaf4xZInK4iPwDuBi437+/UX7/dSLSxt9/QkRG+u95i4hkichxYcc8VUTm+G1viMgH+Usk+WKsBXQGegEZInJ0vu3tRWSqj3OxiHT166uJyPMistT/bf3bf3YXishP+V4jf/xviMi7/m/0LyJyetgxVojIkyJSPuz5J/rX3+D/hm8XkUP9ZxT+o+l0EVkiIna+Ky5VtSWNFmAR0CHfuoeAHbh/+HJAFdzJ9xRcKfJQ4BfgZr9/BUCBpv7xm8A6IAOoCLwDvFmCfesDW4CL/LZ/ADuBvxbwXl4H5gB/BY7It22v4+bb9g7whr//Z+BnXOmjAjAQmFLA8a4F/hP2+ARgPbAfcAEwFajhP8PmwIFFfBf/zf/egCuBH4DD/WfwCDAxLNYpwAH+GMcB9fy294A++V5rHdDG338C2AqcjSt1PQdM8tuqAquB6/1ncJX/3PsUEvuNwEJ/fzLwSNi2o4A/gEv869UHjvfbRgCfAQ38ttMBAS4Efioi/m3AuX7/KkAb/3dUHjgC+BVXOgao47+bXv77qQGc5Ld9DVwddpyXw+O3pRjnk6ADsCXGX2jBCeKrIp53J/Cuvx/ppP9S2L5dgLkl2Pdawk7O/kSwMv9JNGx7VaAfMAPIBeYDnSIdN+w5fXEn8sr+8USge9j2CsB2oFGE49UAsoHG/vGjwDB/vxPwEy6plovyu4iUIKYAXcMeVwZ2+RNeF1xCPAmQfM+LJkG8H7btZGCdv38+8Eu+587K/3r5tn8HPOTv3wAsDb1v4GF8As73nNB7OSzCtmgSxKdFfJ79yEv8N1Bwor8O+NzfrwRsAJon8v8wXRYrcpUdS8MfiMjRIvKJ+AZg4AGgbiHPXxV2PxuoVtCOhex7UHgc6v6DlxX0IqqaraoPqWor3Al0HDBWRGpE2l9EOgN/By5R1W1+9SHAUF/dsRF3UtoNNI5wvE3ABKCriAhwBfCW3/YF8BLwIrBaRF4SkeqFfAYFOQR4JSyeVbjSXWPgI9wv8Jf9MZ4XkarFeO3CPvf8n/NSCiAiR+ES4Vt+1Xu4UsLZ/vHBwMIITz0Il/R/K0bMBcYkIi1EZIKIrPZ/o/eS9zdaUAwA7wKnisiBuFLzr6o6r4QxlWmWIMqO/MP2/guYCxyuqgcAA3D/3PG0krATsz8JN4rmif7k/QjupNc0/3YROQYYDvxFVZeHbVoKXKeqNcOWKqr6fQGHGgV0A9rh/j++DovhGZ+sWuCqmP4RTez5LAWuihBPljpPqGpLXPVWBnBL6PAlOFbIXp+7F7EdxuvubyeLyCrgR1w1T2j9UuCwCM9b4eNsFmHbVlyJEAARqQTUzLdP/vf4Kq40eKj/Gx1E3t9oQTGgqptxybYbcDXwRqT9TNEsQZRd1YFNwFZ/cv1bAo75MdBKRDqLSAXgNqBeQTuLyH0ikiEi+4lIZeBWXK+a+fn2qwl8ANylqv/L9zIvAX39e0REaorIXwqJ8SNcffcAYLQv5SAiJ/ulAu5ktwNXEimul4ABInKkf91aInKpv99WRFr7Y/yR7xircW1FJTEZqCki1/qG8P8Djo20o2/IvRq4C2gZtlwNXOIbf0cCF/vG+/IiUl9EjvOltjeBIX5ded9ALMA8oIGInCki++FKrEWpDmxU1a2+wf36sG3jgOYi0tP/fdQQkYyw7SNxf9MdgLej/JxMPpYgyq47cL8It+BKE+/E+4CquhroCjyFa2A8DJiJaxMoyAi/7wrgTOACVc3Ot08G7qT+nOT1ZNroj/muP967vppiNnBOITFuA95n3xNLTdwv2o24dp6V/nWLRVXfwCWJ9308s4D2fnMt3IltI65BdiHwvN/2EtDW99h5i2Lwn9cluBLPBlybxCQif+5n497rv1R1VWjB/X2sxZXQfvavN8C/3lTyuh/fBCwBsnDf20Bce8pqoLd/nSV+2VhE6LcDN4rIH8CzwOiw97Qe6IhrcF+LK+W0DXvuJFyC+beqriniOKYA4n8gGZNwvsviCtxJZ0rQ8ZQlIjIXuN8n0LQkIlOBp1R1dJE7m4isBGESSkTO9dU8lYD+uO6WUwMOK+2JyNkiUk9EKorI34EmwJdBxxUvInIGrq1qfMChpDS7ktokWjtc1U0F3PUAl6hqYVVMJjaOx1XRVMW14Vyiqr8HG1J8iMh7uGq7v9nfVulYFZMxxpiIrIrJGGNMRCldxVS3bl1t2rRp0GEYY0xKmT59+jpVLbCLeUhKJ4imTZuSmZkZdBjGGJNSRGRxNPtZFZMxxpiI4pYgRGS4uKGR54atqy1uSOX5/raWXy/ihlJeIG4Y5VbxissYY0x04lmCeB03dG+4PsCXqnoErg92aDz683BXwh4B9MQNiGaMMSZAcUsQqvo1btyccBfhhk7A314ctn6kH6zsO9y4MQ3jFZsxxpiiJboNooGqrvT3V+EmFQE3omf4UL/LKGCUTz84V6aIZK5duzZ+kRpjTBkXWCO1HyWz2FfpqeowVc1Q1Yx69YrspWWMMaaEEp0gVoeqjvxtaJTF5ew9Pn1jv84YY0xAEn0dxIe4IaYH+9sPwtbfLCKjcTNZbQqrijLGmLSmCjk5kJ0NW7cWvWRnw4UXwkknxTeuuCUIERmFG7+/rogsA+7DJYYxInIdsBi43O/+KW6M+gW4qRJ7xCsuY5LRrl3uBBFasrP3vo3mfjT75uRApUpQowYccEDkpbBtoe1Vq4LEe/7BJJObG/lEHe0JvajtxR0Wr2HDFE4QqtqtgE3t86/w7RE3xSsWY4pLFXbujO3JubB9d+woWZzly7uTdZUqebeh+9WrQ4MGe6+rXBm2b4fNm2HTJne7ejXMn+/ub97s4ilKuXKlSzCh+/vv714rFlRh27bSnagL26+431GFCu795V9q1ICDDnLfR6Tt+ZdI+1WpErvPrdD3EP9DGBMboRNAvH9lh+7v2lWyOCtVinzCrlIF6tSBgw/ed32kfQu6H76uYsXYfsbgEuOWLXkJJNISadv69fDbb3nbs/PP+xeBiEtkBSWQ6tVh9+7oT+jF/RUefvINv9+wYfQn64L222+/kn3+ycQShIk5VZgxAzZsiP2JvCRECj/h1qpVspNzpPuVK7tf9amsYkWoXdstpZGb6xJNtAkmtGzcCIsX5z0uXz61f4WnMksQJqZyc+G662DkyML3q1Ch4JNspKqR0vzKrlSp7NWXJ4MKFVzyrVUr6EhSkKorki1fDsuWRV4GDoSuXeMahiUIEzPbt8P//R+MGwf33APnnVfwiTweVSPGpITdu13Dz7JlhSeA7fkmwytXztV9NW4Mxx5b+iJeFCxBmJjIzoY//xkmTICnn4bbbw86ImMCsHMnrFxZ+Ml/xQpX1A5XsSI0auRO/iedBJdc4u6HLw0auGJZAlmCMKW2eTN07gxTpsDLL8P11wcdkTFxsG3b3if9SAlg1ap9W8qrVMk7yZ9xxr4n/saNoW7dpGwQsQRhSmX9eleVNHMmvP02XHFF0BEZUwJ//LH3iT7SyX/dun2fV6OGO8E3agTHHRf55F+zZso2glmCMCW2ahV07Oj60I8b50oRxiQVVdctKv/JPn8C2LRp3+fWrZt3kj/llH1P/I0auR4VacwShCmRxYuhQwdXnfrJJ9B+n8sfjYmz3bvdr/pI9fzhCSD/BRkicOCB7iR/5JFw1ln7nvwPOshVDZVxliBMsc2f7xLC5s0wcSK0bRt0RCZqv//uLk5QzVug4Mcl3RbLffMngdDJf/nyfS9vrlDBndwbN4aWLd2ARaHG39DSsKF1o4uSJQhTLHPmuGqlXbtg8mQ48cSgIzIF2r0b5s2Db7+Fb75xtwsWBB1VyVWqlHeSb9t276qe0P369VP/SsUkYgnCRG3qVDj3XFfynjwZjjkm6IjMXrZscV9SKBl8911e3Xr9+u6kesMNEJpHRSRvCX9c2LYg9q1d253869RJ2cbeVGUJwkTl66/hggvcuWXSJDj00KAjKuNUYdEilwhCy+zZrtQgAi1auC5lp53mEsOhh9rJ1RSbJQhTpAkT4NJL4ZBDXHJoFHEyWBNX27e7vsThCWGlnzKlWjVo0wb69XMJ4ZRTXPdLY0rJEoQp1Nix0K2bu7L/iy/yaidMnK1ZA//7X14ymDYtb+iFZs3g7LPzSgctWli9u4kLSxCmQCNHQo8e7gfpp5+6631MHIQ3JocalEONyRUrQuvWcPPNLhmceqrrhWNMAliCMBG98ALcdJPrzvr++64Ww8RIqDE5lAzCG5Pr1XMlg549XUJo3dqNIW5MACxBmH089hjcfbe7MnrMGDs/lYqqu6owvKtppMbktm3dcthh1phskoYlCLOHKvTvDw8/7M5ZI0fa9UTFtmOHa0wOJYOCGpPbtnV1d1ZvZ5KYJQgDuB+0vXvDkCFuNNaXXrJ2z6isXbt3z6JIjcmh0sFxx9mHalKKJQjDrl2uynv4cJcknnzSajkiyt+Y/O23btwRyGtMvummvIRgjckmxVmCKON27ICrr3ZtDQMGuFkMLTl44Y3J337rup2GNya3beuKW6edZo3JJi0FkiBE5DbgBkCAl1X1GREZ6Net9bvdq6qfBhFfWZGTA5dd5kZjffxxuPPOoCMKUHhjcmjJysprTD72WDf/b+jaA2tMNmVAwhOEiLTAJYKTgR3ABBH52G9+WlWfSHRMZdEff0CXLvDvf7v2hr/9LeiIEizUmByeEFascNuqVXMNyNaYbMq4IEoQxwDfq2o2gIj8B7g0gDjKrA0b4PzzXXvqyJFw1VVBR5QAa9fmXZn8zTeQmemmkATXmHzWWXltBy1aJHzuX2OSURD/BXOBh0WkDpADnA9kAuuBm0XkGv/4DlXdkP/JItIT6AnQpEmThAWdLtasgU6d4Mcf4d133dzoaWf3bvcGw689yN+Y/Pe/512ZfNBBwcZrTJISzT/BdiIOKnId8HdgK/ADsB14BFgHKPAg0FBVry3sdTIyMjQzMzPO0aaPZcvcLHBLlrirozt1CjqiGPnjj72HuY7UmBxaMjKsMdmUeSIyXVUzitovkHK0qr4KvAogIoOAZaq6OrRdRF4GPi7g6aYEFi50yeH3392ge+3aBR1RCam6DBdeOojUmNy2rWtQtsZkY0osqF5M9VV1jYg0wbU/tBGRhqrqLznlElxVlImBefNcctixA776ytWwpIwdO2DWrL2vTA41Ju+/v7syuW/fvGGurTHZmJgJqiVurG+D2AncpKobReQ5EWmJq2JaBJS1fjVxMWOGq0qqWBH+8x/3AzuprVu375XJocbkpk3hzDP3HubaGpONiZugqpj+FGHd1UHEks6++cb1VqpVy030c/jhQUeUT3hjcmj55Re3rWJFaNXKGpONCZD9/EpTEyfCxRe7qXwnTYKDDw46IvIak8OvTN640W2rW9eVDK67Lm+Y6ypVgo3XmDLOEkQa+uADuPxyOPpo1yDdoEEAQYQ3JodfmbxrV15j8uWX5/UuOvxwa0w2JslYgkgzb78N11zjenN++inUrh1AELNmueLL4sXucagx+d57XTJo08Yak41JAZYg0siwYdCrF5xxBnz4IVSvHkAQqnDrrW6gp+efzxvm2hqTjUk59l+bJp580g22d/758N57AVbff/wxTJkCL77ospUxJmWVCzoAUzqqbojuO+90I7OOHx9gcsjNdXOVHnWUa2w2xqQ0K0GkMFWXGJ56Cnr0gJdfDnjCstdfd91Wx42zuUqNSQNWgkhRu3a5IbqfegpuuQVeeSXg5JCdDffd59ocLr44wECMMbFiJYgUtHMn/PWvrsfSvffCQw8lQQ/RZ55xQ2CMGZMEwRhjYsESRIrZtg2uuMJd6/DII9CnT9AR4eZaGDzYlRxOOy3oaIwxMWIJIoVs3erOwZMmuR6kN90UdETeQw+5KqZHHgk6EmNMDFmCSBEbN8IFF8B337m24O7dg47I+/VX16X1uuvcpdvGmLRhCSIFrF0L55wDc+fCO+/AX/4SdERh+vZ1PZYGDgw6EmNMjFmCSHIrVri5HH77zbU7nHde0BGFmTYNRo+G/v2hYcOgozHGxJgliCT2228uOaxZAxMmuCE0koaquyiuXj345z+DjsYYEweWIJLUTz+55JCdDV9+CSefHHRE+UyYAJMnw3PPBTTokzEm3ixBJKFZs9wscOXKuVngjjsu6Ijy2bUL7rrLzffcs2fQ0Rhj4sQSRJL53//cgHvVq7vurEceGXREEbzxhmsxHzMG9tsv6GiMMXFiQ20kka++go4d3eRqU6YkaXLIyXGN0iefnGTdqYwxsWYliCTx8cfufHvEEW4WuKTtFPTcc7BsGbz5pg2pYUyasxJEEhgzBi65xLU1/PvfSZwc1q+HQYPcFXtJ1aXKGBMPliACNnw4dOsGp57qeivVqRN0RIUYNAi2bHHjLhlj0l4gCUJEbhORuSLyg4jc7tfVFpGJIjLf39YKIrZEGjLEjVDRsaPrNXrAAUFHVIhFi9wAUH/9K7RoEXQ0xpgESHiCEJEWwA3AycAJwIUicjjQB/hSVY8AvvSP05IqPPww3HYbXHqpu0K6atWgoypC//6u3+399wcdiTEmQYpMECJSRUTuEZGX/OPDRaQ0Az4cA3yvqtmqmgv8B7gUuAgY4fcZAaTlrDOqcM890K8fXH21G1upUqWgoyrCzJmuUfr226Fx46CjMcYkSDQliOGAAO384xXAoFIccy7wJxGpIyJVgfOBg4EGqrrS77MKaBDpySLSU0QyRSRz7dq1pQgj8XbvhptvhkcfhRtvdKOyVkiFfmR33+0aR5Ji8gljTKJEkyCOUNVBwE4AVc3GJYwSUdUfgUeBL4AJwCxgV759FNACnj9MVTNUNaNevXolDSPhcnPdvNEvvOAuQh461NXYJL2JE93Srx/UqBF0NMaYBIrmFLVDRCrjT9gi0gzYUZqDquqrqtpaVU8HNgC/AKtFpKE/RkNgTWmOkUy2b4euXWHkSDe3zuDBKXIJwe7dLps1beqKPMaYMiWaCo4Hcb/0G4vICOAM4LrSHFRE6qvqGhFpgmt/aAM0A7oDg/3tB6U5RrLIzoY//9n1UnrmGdcwnTLeftsNDPXWWynQUGKMiTVxtTlF7CRSD2iLq1r6VlVL9eteRKYAdXDVVv9Q1S9FpA4wBmgCLAYuV9XfC3udjIwMzczMLE0ocbV5M3Tu7IbNeOUVuPbaoCMqhm3b3Axxdeq4eR9Soj7MGBMNEZmuqhlF7VdkCUJEPgeeVNUPwta9oKp/L2lwqvqnCOvWA+1L+prJZv16N7nPzJkwapSrYkopL7wAixfDq69acjCmjIrmP/9IYICI9A1b1yZO8aSFVavgzDNh9mwYNy4Fk8OGDa6x5JxzoH3a5GxjTDFFkyA2AGcBh4jI+yJis8MUYvFi+NOf3Gxwn3ziqphSzuDBsHGj649rjCmzokkQoqo7VbUn8AnwDZA6/UsTaP58lxzWrnU9Q1Pyx/fSpfDss+4qvhNOCDoaY0yAounF9HLojqq+LCJZwM3xCyk1zZnjxlTavduNyNqyZdARldCAAe72wQeDjcMYE7gCSxAisr+/+6aIHBBagJ+wBLGXqVPd6NcVKsDXX6dwcpgzB0aMgFtugSZNgo7GGBOwwkoQ7wHnAT/gLpILv7RLcd1Ry7z//AcuvBDq1XPDdTdrFnREpXD33e5q6XvuCToSY0wSKDBBqOp5/vbgxIWTWiZMcBP9NGvm2hwaNQo6olKYPBk++wwefxxq1w46GmNMEohmNNc2flA9RKSbiDwmImU+aYwdC126wDHHuFJESieH0JAaTZq40QSNMYboejENA3JE5HjgbmA58EZco0pyI0bA5ZfDSSfBV1+56qWU9u67kJnpGqYrVw46GmNMkogmQeT60VUvAp5X1WeBZJ77LK5eeMFNqnbWWfDFF1CzZtARldKOHXDvvXD88XDllUFHY4xJItF0c90qIv8ErgLOFJFyQMX4hpWcHn3UTYnQpYub6Cctfmy/9BL8+qtrUClfPuhojDFJJJoSRFdcD6ZefkKfxsBTcY0qyahC374uOXTrBu+9lybJYfNmV63Uvj106hR0NMaYJFNkCUJVVwCPhT1eArwWz6CSye7d0Ls3DBkCN9wAL76YRj+0H3sM1q1zRaOUmKDCGJNIqTDhZWB27XJJ4bXX4B//gCeeSKPz6PLl8NRTrkjUunXQ0RhjkpCN41yAHTvcufO11+C++9IsOQAMHOjmQX344aAjMcYkKStBRJCTA5dd5kZjfeIJuOOOoCOKsXnzYPhwuPXWFL/02xgTT9FMGHQY8DDQHNjTNKuqR8YxrsBs2QIXXeQG3PvXv6Bnz6AjioN77oFq1VzLuzHGFCCaKqbXcY3SghubaQzwThxjCsyGDW5E1q+/hjfeSNPkMGUKfPih65JVt27Q0Rhjklg0CaKqqn4OoKoLVbUfLlGkldWr3SxwM2e6bqxpec2YqhtSo1EjuO22oKMxxiS5aNogtvuL4xaKSC/cUBtpNavc0qXQoQMsWwYff+xKEWlp3Dj47js3z3TVqkFHY4xJctEkiN7A/sCtuLaIA4Ae8QwqkRYudNeJbdgAn38O7doFHVGc7Nzp2h6OPRa6dw86GmNMCogmQTRS1e+BLcDVACJyaVyjSpAffnClhR073KB7aX05wMsvuzlRP/ooja70M8bEUzRtEP0irCtV9xcR6S0iP4jIXBEZJSKVReR1EflNRGb5Ja7zsk2f7maBAzdcd1onhy1b4P773Ru+4IKgozHGpIgCSxAicg5wLtBIRMLHXjoA2F3SA4pII1x1VXNVzRGRMcAVfvM/VfW9kr52tP77X3eerFXLzQJ32GHxPmLAnnwS1qxxpYe0utrPGBNPhVUxrQHmAttw046GbAH6xOC4VURkJ1AVWFHK1yuWRYtcR54vvoDGjRN55ACsWuWu9rvsMjj55KCjMcakEHFTPRSyg0hlVd0W04OK3IZr8M4BvlDVK0XkdeBUYDvwJdBHVbdHeG5PoCdAkyZNWi9evLhEMWzfDpUqlSz+lHLjjfDKK/Djj3D44UFHY4xJAiIyXVUzitovmjaIRiIyWkRmi8gvoaUUgdXCTT7UDDgI2F9ErgLuAY4GTgJq42av24eqDlPVDFXNqFeKqdzKRHL4+WfXON2rlyUHY0yxBXEldQfgN1Vdq6o7gXFAW1Vdqc52fzyrDymte++FKlWgf/+gIzHGpKAgrqReArQRkaoiIkB74EcRaQjg112Ma/8wJfW//7kL4+66C+rXDzoaY0wKSviV1Kr6vYi8B8wAcoGZwDDgMxGphyupzAJ6lfQYZZ4q/POfcOCBbiILY4wpgZJcSV0DuLY0B1XV+4D78q0+uzSvacJ8+CF8840bjnb//YOOxhiToorsxZTMMjIyNDMzM+gwkktuLhx3nLs/Zw5UsCk/jDF7i7YXU2EXyo0HCsweqpoWw22knddeg59+gvHjLTkYY0qlsDPI8/72Ilx31Lf8424k+MI2E6WtW2HAAGjb1s16ZIwxpVBgglDVLwFE5NHwooiIvA9MTUBsprieftpdOT12rA2pYYwptWi6uVYTkaZhj5sA1eISjSm5tWvhscfgkktcCcIYY0opmkrqO4ApIvIzrgvq4VgX1OTz4IOQnQ2PPBJ0JMaYNFFkglDVT0TkSKC5XzVPVXPiG5YplgUL4MUX4frr4aijgo7GGJMmourm4hPC9DjHYkqqb183uNTAgUFHYoxJI9G0QZhkNm0ajBkDd9zhrpw2xpgYsQSRykJDatSvD3feGXQ0xpg0U2QVk4gcH2H1JmCpqpZ4ZjkTA59+6uZLff55qF7i4bGMMSaiaNogXgVa4maVE+AYYB5QXUR6hq6XMAm2axf06QNHHAE9ewYdjTEmDUVTxbQIaK2qLVX1BKA18AtwDvBkHGMzhRk5EubOhUGDoGLFoKMxxqShaBLEMao6O/RAVecAzVV1QfzCMoXKyXGTAJ1yCvz5z0FHY4xJU9FUMf0kIs8Bo/3jrn5dJdx8DibRnn0Wli+Ht9+2ITWMMXETTQniGmAZ0McvK4DuuOTQPn6hmYjWr4fBg6FzZzj99KCjMcaksWiupM4GHvVLfptiHpEp3MMPw5YtLkkYY0wcRdPNtQ1u9rdDwvdX1SPjGJeJ5LffYOhQ6NEDmjcven9jjCmFaNogXgPuwg21sSu+4ZhC9esH5cvD/fcHHYkxpgyIJkFsVtWP4h6JKdyMGa5R+t57oVGjoKMxxpQB0SSIr0TkEWAcsD20Mrzrq0mAu+9K6BI8AAAaP0lEQVSGOnXgrruCjsQYU0ZEkyDa5bsFN1e1daFJlC++gEmT4JlnoEaNoKMxxpQR0fRi+lOsDyoivYHrcYlmDtADaIi71qIOrr3jalXdEetjp5zdu12poVkz6GXzNBljEqfABCEi3VR1lIjcGmm7qg4pyQFFpBFwK+5q7BwRGQNcAZwPPK2qo0XkJeA64MWSHCOtvPUWZGXBqFFuzgdjjEmQwi6Uq+Vv6xWwlEYFoIqIVACqAiuBs4H3/PYRwMWlPEbq27bN9Vxq3RouvzzoaIwxZUyBJQhVfcHf9o/lAVV1uYg8ASwBcoAvcFVKG1U1NHTHMsC66gwdCkuWwGuvQTmbusMYk1jRXChXF7gWaMreF8qVaIxpEakFXAQ0AzYC7wLnFuP5PYGeAE2aNClJCKlhwwZ31fS558LZZwcdjTGmDIqmF9MHwHfAf4nNhXIdgN9UdS2AiIwDTgNqikgFX4poDCyP9GRVHQYMA8jIyNAYxJOcHnkENm6ERyONcGKMMfEXTYLYX1XviOExlwBtRKQqroqpPZAJTAb+guvJ1B2XmMqmJUtgyBC45ho4PtKEfsYYE3/RVGx/JiKdYnVAVf0e1xg9A9fFtRyuRHA38A8RWYDr6vpqrI6ZcgYMcLcPPBBsHMaYMk1UC6+lEZENQA0gG9iBm3ZUVbV2/MMrXEZGhmZmZgYdRmxlZcGJJ8Kdd8JjjwUdjTEmDYnIdFXNKGq/aKqY6sYgHhOtPn2gZk24556gIzHGlHGFXSh3hKrOB44tYBcbiynWvvoKJkyAJ56AWrWK3t8YY+KosBJEH9zVzEMjbLOxmGItNKRGkyZw001BR2OMMYVeKHedv435WEwmgnfegenTYeRIqFw56GiMMSaqNghE5GigObDnzKWqb8crqDJn+3bo2xdOOAGuvDLoaIwxBojuSup+QCfgaOBz4BzcRXOWIGLlpZfcdKKff25DahhjkkY0Z6OuwFnASlW9GjgB2D+uUZUlmzbBgw9Chw7QKWaXmxhjTKlFkyByVHUXkCsi1YFVwCHxDasMefRRWL/ehtQwxiSdaNogZopITWA4bkiMzcDUuEZVVixf7maJu/JKaNUq6GiMMWYvhSYIERFgoKpuBIaKyOfAAao6IyHRpbv77oNdu+Chh4KOxBhj9lFoFZO6cTgmhj1eYMkhRn74wc3zcNNN0LRp0NEYY8w+ommDmCUiJ8Y9krKmTx+oXt11bzXGmCRU2FAbobkZTgSmichCYCt5g/VZpXlJff01fPwxDB4MdeoEHY0xxkRUWBvEVKAV0CVBsZQNqm5IjcaN4dZbg47GGGMKVFiCEABVXZigWMqGsWPh++9h+HCoUiXoaIwxpkCFJYh6IvKPgjaq6lNxiCe97dzphvFu0cLNFmeMMUmssARRHqiGL0mYGBg2DBYsgE8+gfLlg47GGGMKVViCWKmqNudlrGzZAvffD2eeCeedF3Q0xhhTpMK6uVrJIZaeeALWrnXTiIp9tMaY5FdYgmifsCjS3cqVLkFcfjmcdFLQ0RhjTFQKTBCq+nsiA0lr99/vGqgHDQo6EmOMiZpNPhBvP/8Mr7wCvXrBYYcFHY0xxkTNEkS83XMPVK0K/fsHHYkxxhRLVFOOxpKIHAW8E7bqUGAAUBO4AVjr19+rqp8mOLzY+uYbGD/eTQhUr17Q0RhjTLGIG7A1oIOLlAeWA6cAPYA/VPWJaJ+fkZGhmZmZ8QqvdFShXTs3lej8+bC/TcJnjEkOIjJdVTOK2i/hJYh82gMLVXWxpFvXzw8+gG+/dRfHWXIwxqSgoNsgrgBGhT2+WURmi8hwEakV6Qki0lNEMkUkc+3atZF2CV5urhvO++ijoUePoKMxxpgSCSxBiMh+uJFi3/WrXgQOA1oCK4EnIz1PVYepaoaqZtRL1nr9V191vZcGD4YKQRfSjDGmZIIsQZwHzFDV1QCqulpVd6nqbuBl4OQAYyu5rVth4EDX/tDFRko3xqSuIH/ediOseklEGqrqSv/wEmBuIFGV1lNPwapVMG6cDalhjElpgSQIEdkf6Aj8LWz1YyLSElBgUb5tqWHNGjfW0qWXwqmnBh2NMcaUSiAJQlW3AnXyrbs6iFhi6oEHICfHhtQwxqSFoHsxpY/58+Ff/4KePeGoo4KOxhhjSs0SRKz07QuVKsF99wUdiTHGxIQliFj4/nt49124805o0CDoaIwxJiYsQZSWKtx1F9SvD3fcEXQ0xhgTM3YVV2l98gl8/TW88AJUrx50NMYYEzNWgiiNXbvckBpHHgnXXx90NMYYE1NWgiiNESPghx/gvfegYsWgozHGmJiyEkRJZWe7SYDatHEXxhljTJqxEkRJPfssrFgBo0fbkBrGmLRkJYiSWLfOjdTapQv86U9BR2OMMXFhCaIkHn4Y/vgDHnkk6EiMMSZuLEEU16+/wtChcO210Lx50NEYY0zcWBtEcfXr5yYBuv/+oCMxJunt3LmTZcuWsW3btqBDKZMqV65M48aNqVjCXpaWIIpj+nQYNcqNu3TQQUFHY0zSW7ZsGdWrV6dp06ak3bzzSU5VWb9+PcuWLaNZs2Yleg2rYopWaEiNunXdrTGmSNu2baNOnTqWHAIgItSpU6dUpTcrQUTr88/hq69c99YDDgg6GmNShiWH4JT2s7cSRDR27YK774ZDD4VevYKOxhhjEsISRDTeegtmz3Yzxe23X9DRGGOisH79elq2bEnLli058MADadSo0Z7HO3bsiOo1evTowc8//1zoPkOHDuWtt96KRchJR1Q16BhKLCMjQzMzM+N7kG3b3GB8DRq4eR/KWU41Jlo//vgjxxxzTNBhMHDgQKpVq8add96513pVRVUpl8b/15G+AxGZrqoZRT3X2iCK8txzsHSpG5gvjf+IjIm322+HWbNi+5otW8IzzxTvOQsWLKBLly6ceOKJzJw5k4kTJ3L//fczY8YMcnJy6Nq1KwMGDACgXbt2PP/887Ro0YK6devSq1cvPvvsM6pWrcoHH3xA/fr16devH3Xr1uX222+nXbt2tGvXjq+++opNmzbx2muv0bZtW7Zu3co111zDjz/+SPPmzVm0aBGvvPIKLVu23Cu2++67j08//ZScnBzatWvHiy++iIjwyy+/0KtXL9avX0/58uUZN24cTZs2ZdCgQYwaNYpy5cpx4YUX8vDDD8fqowWsiqlwv//uqpXOPx/OOivoaIwxMfLTTz/Ru3dv5s2bR6NGjRg8eDCZmZlkZWUxceJE5s2bt89zNm3axBlnnEFWVhannnoqw4cPj/jaqsrUqVN5/PHHeeCBBwB47rnnOPDAA5k3bx79+/dn5syZEZ972223MW3aNObMmcOmTZuYMGECAN26daN3795kZWXx7bffUr9+fT766CM+++wzpk6dSlZWFnfEYcIyK0EU5pFHYNMmN+6SMaZUivtLP54OO+wwMjLyalhGjRrFq6++Sm5uLitWrGDevHk0zzdSQpUqVTjvvPMAaN26NVOmTIn42pf60Z1bt27NokWLAPjvf//L3XffDcAJJ5zAscceG/G5X375JY8//jjbtm1j3bp1tG7dmjZt2rBu3To6d+4MuIvfACZNmsS1115LlSpVAKhdu3ZJPopCJTxBiMhRwDthqw4FBgAj/fqmwCLgclXdkOj49li8GIYMge7d4bjjAgvDGBN7+++//5778+fP59lnn2Xq1KnUrFmTq666KuK1A/uFdVApX748ubm5EV+7UqVKRe4TSXZ2NjfffDMzZsygUaNG9OvXL/Ar0BNexaSqP6tqS1VtCbQGsoHxQB/gS1U9AvjSPw5O//6uzcEXEY0x6Wnz5s1Ur16dAw44gJUrV/L555/H/BinnXYaY8aMAWDOnDkRq7BycnIoV64cdevWZcuWLYwdOxaAWrVqUa9ePT766CPAXXyYnZ1Nx44dGT58ODk5OQD8/vvvMY876Cqm9sBCVV0sIhcBZ/r1I4B/A3cHElVWFrz5prti+uCDAwnBGJMYrVq1onnz5hx99NEccsghnHbaaTE/xi233MI111xD8+bN9yw1atTYa586derQvXt3mjdvTsOGDTnllFP2bHvrrbf429/+Rt++fdlvv/0YO3YsF154IVlZWWRkZFCxYkU6d+7Mgw8+GNO4A+3mKiLDgRmq+ryIbFTVmn69ABtCj/M9pyfQE6BJkyatFy9eHPvAzj0Xpk2DhQuh5j4hGGOilCzdXIOWm5tLbm4ulStXZv78+XTq1In58+dToUL8f6OnZDdXEdkP6ALck3+bqqqIRMxcqjoMGAbuOoiYBzZpkhtW48knLTkYY2Lijz/+oH379uTm5qKq/Otf/0pIciitICM8D1d6WO0frxaRhqq6UkQaAmsSHtHu3a5a6ZBD4KabEn54Y0x6qlmzJtOnTw86jGIL8jqIbsCosMcfAt39/e7ABwmPaPRomDnTzRjneyIYY0xZFUiCEJH9gY7AuLDVg4GOIjIf6OAfJ8727W6ehxNPhG7dEnpoY4xJRoFUManqVqBOvnXrcb2agvHCC7BoEQwbZkNqGGMMNtSGs3EjPPQQdOzoFmOMMZYgAHj0Udiwwd0aY9JCLIb7Bhg+fDirVq3a8ziaIcDTRfL3s4q3ZcvcIDFXXunaH4wxaaFOnTrM8sPHFjTcdzSGDx9Oq1atOPDAAwF47bXXYhpnMrMEMWCA694a4ysQjTH5JMt438CIESMYOnQoO3bsoG3btjz//PPs3r2bHj16MGvWLFSVnj170qBBA2bNmkXXrl2pUqUKU6dO5eyzzy5yCPD58+dz1VVXkZ2dTZcuXRg6dCgbN27cJ47OnTuzYsUKtm3bRu/evbn++usB+OSTT+jfvz+7du2iQYMGfPHFF2zZsoWbb755z0iwDzzwABdffHHpPr8ilO0qprlz3TwPN98MTZsGHY0xJgHmzp3L+PHj+fbbb5k1axa5ubmMHj2a6dOns27dOubMmcPcuXO55ppr6Nq1Ky1btuSdd95h1qxZew3YBwUPAX7LLbdw5513MmfOHBo2bFhgLCNGjGD69OlMmzaNp556ig0bNrBq1SpuvPFGxo8fT1ZWFqNHjwZcKahevXrMnj2brKwszjjjjPh9SF7ZLkH06QMHHOC6txpj4itJxvueNGkS06ZN2zPcd05ODgcffDDnnHMOP//8M7feeisXXHABnTp1KvK1ChoC/Pvvv+fTTz8F4P/+7//o169fxOc//fTTfPjhhwAsW7aMhQsXsnTpUs466ywOOeQQIG8Y70mTJvH+++8DICLUqlWrpB9B1Mpugvj3v+GTT1zDdBzGUTfGJCdV5dprr404sN3s2bP57LPPGDp0KGPHjmXYsGGFvla0Q4BHMmnSJL7++mu+++47qlSpQrt27QIf3ju/slnFpOqG1GjcGG65JehojDEJ1KFDB8aMGcO6desA19tpyZIlrF27FlXlsssu44EHHmDGjBkAVK9enS1bthTrGCeffDLjx48H2FNFlN+mTZuoXbs2VapU4YcffmDatGkAtG3blsmTJxMaiDQ0jHfHjh0ZOnQo4JLchg3xny6nbCaId991o7U++CD42ZiMMWXDcccdx3333UeHDh04/vjj6dSpE6tXr2bp0qWcfvrptGzZkh49ejBo0CDAdWu9/vrri9U9dsiQITz66KMcf/zx/Pbbb/sM7Q1wwQUXkJ2dTfPmzenXr9+e4b0bNGjAiy++yEUXXcQJJ5zAlVdeCbj5qlevXk2LFi1o2bJlgTPaxVKgw32XVkZGhmZmZhb/iZ9+6q6YHjsWypePfWDGGKDsDve9detWqlatiojw5ptvMn78+D0TACVaSg73Hajzz3eLMcbEwbRp07j99tvZvXs3tWrVStlrJ8pmgjDGmDg688wz91ykl8rKZhuEMSZhUrkaO9WV9rO3BGGMiZvKlSuzfv16SxIBUFXWr19P5cqVS/waVsVkjImbxo0bs2zZMtauXRt0KGVS5cqVady4cYmfbwnCGBM3FStWpFmzZkGHYUrIqpiMMcZEZAnCGGNMRJYgjDHGRJTSV1KLyFpgI7CpBE+vC6wrxv41SnicWL1OcZ8X7f5F7VfS7cX9fIMWq+83kccp6Ws1AZbE+BjR7FfYPoVts7+lyIrzPeZ3iKrWK3IvVU3pBRhWwudlJuI4MYy3WM+Ldv+i9ivp9uJ+vkEvsfp+E3mcUvwtrY31MaLZr7B9ithmf0ul/B5LuqRDFdNHKXackr5OcZ8X7f5F7Vfa7aki1f6OSvNa+05tVvpjRLNfYfuky98RJO69FOd7LJGUrmIqDRHJ1CgGqzIlY59v8kq17ybV4k2URHwu6VCCKKnCZwIxpWWfb/JKte8m1eJNlLh/LmW2BGGMMaZwZbkEYYwxphCWIIwxxkSUtglCRA4WkckiMk9EfhCR2/z6gSKyXERm+eX8sOfcIyILRORnETknuOiTk4gMF5E1IjI3bF1tEZkoIvP9bS2/XkRkiP88Z4tIq7DndPf7zxeR7kG8l3QjIotEZI7/m87065Lmu4n3346ItPbvf4F/rsQq9ngSkcoiMlVEsvx56n6/vpmIfO/fzzsisp9fX8k/XuC3Nw17rYjnLxE5169bICJ9ihVg0H2G49hHuCHQyt+vDvwCNAcGAndG2L85kAVUApoBC4HyQb+PZFqA04FWwNywdY8Bffz9PsCj/v75wGeAAG2A7/362sCv/raWv18r6PeW6guwCKibb13SfDfx/tsBpvp9xT/3vKC/kyg/FwGq+fsVge/9+xgDXOHXvwTc6O//HXjJ378CeMffj3j+8stC4FBgP79P82jjS9sShKquVNUZ/v4W4EegUSFPuQgYrarbVfU3YAFwcvwjTR2q+jXwe77VFwEj/P0RwMVh60eq8x1QU0QaAucAE1X1d1XdAEwEzo1/9GVS0nw38fzb8dsOUNXv1J0tR4a9VlLz7/EP/7CiXxQ4G3jPr8//2YQ+s/eA9r60VND562Rggar+qqo7gNF+36ikbYII54thJ+KyM8DNvug6PFSsxSWPpWFPW0bhCcU4DVR1pb+/Cmjg7xf0edrnHB8KfCEi00Wkp1+X7N9NrOJr5O/nX58SRKS8iMwC1uCS3kJgo6rm+l3C38+ez8Bv3wTUIU7fadonCBGpBowFblfVzcCLwGFAS2Al8GSA4aUV/+vN+k0Ho52qtgLOA24SkdPDNyb7d5Ps8cWTqu5S1ZZAY9wv/qMDDmmPtE4QIlIRlxzeUtVxAKq62n8hu4GXyatGWg4cHPb0xn6dKdxqX8TH367x6wv6PO1zjgNVXe5v1wDjcX/Xyf7dxCq+5f5+/vUpRVU3ApOBU3HVaqEJ3cLfz57PwG+vAawnTt9p2iYIXy/3KvCjqj4Vtr5h2G6XAKFeFR8CV/heAs2AI3ANX6ZwHwKh3iTdgQ/C1l/je6S0ATb56oTPgU4iUstX73Xy60wJicj+IlI9dB/3mc4l+b+bmMTnt20WkTb+//6asNdKaiJST0Rq+vtVgI649tLJwF/8bvk/m9Bn9hfgK1/6Kuj8NQ04wveK2g/XsP1h1AEG3YofrwVohyuyzgZm+eV84A1gjl//IdAw7Dl9cfV/P5MivSAS/JmOwlXL7cTVZV6Hq//8EpgPTAJq+30FGOo/zzlARtjrXItrRFsA9Aj6faX6guuhkuWXH4C+fn3SfDfx/tsBMnBJcSHwPH6UiGRfgOOBmf58NBcYEPadTvXv812gkl9f2T9e4LcfGvZaEc9f/rz3i9/Wtzjx2VAbxhhjIkrbKiZjjDGlYwnCGGNMRJYgjDHGRGQJwhhjTESWIIwxxkRkCcKkLRG5WERURJLmytTiEJEqIvIfPxTDUX4YjdkicqrfXkFEJolI1bDnjBaRI4KL2qQTSxAmnXUD/utv40ZEysfppa8FxqnqLuBvwG24Pu13+u03Am+qanbYc14E7opTPKaMsQRh0pIfg6sd7oKsK/Jtu9vPHZAlIoP9usP9r/EsEZkhIoeJyJki8nHY854Xkb/6+4tE5FERmQFcJiI3iMg0//yxoV/1ItJARMb79Vki0lZEHhCR28Ne92Hx85XkcyV5V9DuBKr6Zae/+rYzbuTScFOADmHDNBhTYvZHZNLVRcAEVf1FRNaLSGtVnS4i5/ltp6hqtojU9vu/BQxW1fEiUhn34+ngAl47ZL26AfIQkTqq+rK//xAuMT0HDAH+o6qX+JJGNWAFMA54RkTK4RLYXkPL+2ERDlXVRX7VUFwyqIQrTfQHBqkbU2wPVd0tIguAE4Dp0X9cxuzLShAmXXXDjX2Pvw1VM3UAXgtVy6jq734co0aqOt6v25av2qYg74TdbyEiU0RkDu6X/7F+/dm4ah/UDRK5yZ/014vIibjxhGaq6vp8r10X2Bh6oKpLVPVMVT0VyMYNuvajiLwhboaxI8OeuwY4KIr4jSmUlSBM2vGlgrOB40REcbNqqYj8s5gvlcveP6Iq59u+Nez+68DFqprlq6HOLOK1XwH+ChwIDI+wPSfC8UIeBvoBt/rXWQQMwiWmUJw5RRzfmCJZCcKko78Ab6jqIaraVFUPBn4D/oSbkKVHWBtBbXUzDi4TkYv9ukp++2KguX9cE2hfyDGrAyvFDTF/Zdj6L3GNyaGJYWr49eNxs7WdRIQRU9XNmFbeV3ftISJnACtUdT6uPWK3X6qG7XYkeaMUG1NiliBMOuqGOwGHGwt0U9UJuFF8M8XN4hXqEXQ1cKuIzAa+BQ5U1aW4uYHn+tuZhRyzP27Gwm+An8LW3wac5auepuPmDkbd9I+TgTG+l1IkX+Aa2oE9Q9j3Ax70q4YBzwKfAE/4fRoAOaq6qpBYjYmKjeZqTAB84/QM4DJfGoi0Tyugt6peXYzX7Q1sVtVXYxOpKcusBGFMgolIc9x4/l8WlBwAVHUGMLmY11lsJG9Se2NKxUoQxhhjIrIShDHGmIgsQRhjjInIEoQxxpiILEEYY4yJyBKEMcaYiP4fUsD7DP3NfRwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sizes = [250, 1000, 5000, 10000, 30000]\n",
    "\n",
    "plt.plot(np.asarray(sizes), np.asarray(training_acc_arr), \"-b\", label=\"Training acc\")\n",
    "plt.plot(np.asarray(sizes), np.asarray(testing_acc_arr), \"-r\", label=\"Testing acc\")\n",
    "plt.xscale(\"log\")\n",
    "plt.xticks(sizes,labels=sizes)\n",
    "plt.title(\"Training Size vs Testing Accuracy\")\n",
    "plt.xlabel('Accuracy (%)')\n",
    "plt.ylabel('Training data size')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "As expected, when the training dataset size increases, the model's accuracy on the training and testing data increases. Based on the above graph, training and testing accuracies start to plateau at around 1000. Data sizes larger than 1000 up until 30000 appear to make marginal changes to the accuracies.\n",
    "\n",
    "Thus, my worst model was the one trained on 250 data points; whereas, the best was the one trained on 30000 data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.  PyTorch System Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, data_path, csv_name, transform = None ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_path (string): path to the folder where images and csv files are located\n",
    "            csv_name (string): name of the csv lablel file\n",
    "            transform: pytorch transforms for transforms and tensor conversion\n",
    "        \"\"\"\n",
    "        # Set path\n",
    "        self.data_path = data_path\n",
    "        # Transforms\n",
    "        self.transform = transform\n",
    "        # Read the csv file\n",
    "        self.data_info = pd.read_csv(data_path + csv_name, header=None)\n",
    "        # First column contains the image paths\n",
    "        self.image_arr = np.asarray(self.data_info.iloc[:, 0])\n",
    "        # Second column is the labels\n",
    "        self.label_arr = np.asarray(self.data_info.iloc[:, 1])\n",
    "        # Calculate len\n",
    "        self.data_len = len(self.data_info.index)\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get image name from the pandas df\n",
    "        single_image_name = self.image_arr[index]\n",
    "        # Open image\n",
    "        img_as_img = Image.open(self.data_path + single_image_name)\n",
    "        if self.transform is not None:\n",
    "              img_as_img = self.transform(img_as_img)\n",
    "\n",
    "        # Get label(class) of the image based on the cropped pandas column\n",
    "        single_image_label = self.label_arr[index]\n",
    "        #convert to tensor to be consistent with MNIST dataset\n",
    "        single_image_label = torch.LongTensor( [ single_image_label ] )[0]\n",
    "        return (img_as_img, single_image_label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydataT = SimpleDataset( \"./idata/\", \"labels.csv\", transform=transforms.ToTensor())\n",
    "\n",
    "my_test_loader = torch.utils.data.DataLoader(dataset=mydataT,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy of model_250: 15.00%\n"
     ]
    }
   ],
   "source": [
    "model_250.eval() \n",
    "with torch.no_grad():  \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in my_test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model_250(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(\"Testing accuracy of model_250: {:.2f}%\".format(100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing accuracy of model_30000: 36.67%\n"
     ]
    }
   ],
   "source": [
    "model_30000.eval() \n",
    "with torch.no_grad():  \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in my_test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model_30000(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(\"Testing accuracy of model_30000: {:.2f}%\".format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "As shown above, my worst model tested worse at 15% (9 of 60) than the best model at 36.67% (22 of 60). Since the best model had much more training data to work with (although there is a possiblity of overfitting), it was able to predict my test data better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
