{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Convolutional Neural Networks using Keras-Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "'''Trains a simple convnet on the MNIST dataset.\n",
    "Gets to 99.25% test accuracy after 12 epochs\n",
    "(there is still a lot of margin for parameter tuning).\n",
    "16 seconds per epoch on a GRID K520 GPU.\n",
    "'''\n",
    "num_classes = 10\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/6\n",
      "60000/60000 [==============================] - 120s 2ms/step - loss: 0.2484 - accuracy: 0.9234 - val_loss: 0.0572 - val_accuracy: 0.9813\n",
      "Epoch 2/6\n",
      "60000/60000 [==============================] - 121s 2ms/step - loss: 0.0889 - accuracy: 0.9739 - val_loss: 0.0415 - val_accuracy: 0.9860\n",
      "Epoch 3/6\n",
      "60000/60000 [==============================] - 121s 2ms/step - loss: 0.0633 - accuracy: 0.9808 - val_loss: 0.0381 - val_accuracy: 0.9871\n",
      "Epoch 4/6\n",
      "60000/60000 [==============================] - 121s 2ms/step - loss: 0.0508 - accuracy: 0.9844 - val_loss: 0.0372 - val_accuracy: 0.9880\n",
      "Epoch 5/6\n",
      "60000/60000 [==============================] - 117s 2ms/step - loss: 0.0422 - accuracy: 0.9872 - val_loss: 0.0307 - val_accuracy: 0.9899\n",
      "Epoch 6/6\n",
      "60000/60000 [==============================] - 121s 2ms/step - loss: 0.0351 - accuracy: 0.9892 - val_loss: 0.0277 - val_accuracy: 0.9903\n",
      "Test loss: 0.027688994653394913\n",
      "Test accuracy: 0.9902999997138977\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 6\n",
    "model = Sequential([\n",
    "    Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    Dropout(0.25),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "hist = model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "y_predict = model.predict(x_test, verbose=0)\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 977    0    1    0    0    0    0    1    1    0]\n",
      " [   0 1132    0    2    0    0    1    0    0    0]\n",
      " [   0    2 1023    0    0    0    0    5    2    0]\n",
      " [   0    0    2 1003    0    2    0    1    2    0]\n",
      " [   0    0    1    0  971    0    4    0    2    4]\n",
      " [   1    0    0    4    0  886    1    0    0    0]\n",
      " [   7    2    0    1    1    3  942    0    2    0]\n",
      " [   1    2    4    2    0    0    0 1015    1    3]\n",
      " [   4    0    1    0    0    0    0    1  966    2]\n",
      " [   2    1    0    2    3    5    0    4    4  988]]\n"
     ]
    }
   ],
   "source": [
    "y_predict_amax = np.array([np.argmax(y) for y in y_predict])\n",
    "y_test_amax = np.array([np.argmax(y) for y in y_test])\n",
    "\n",
    "matrix = confusion_matrix(y_test_amax,y_predict_amax)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/6\n",
      "60000/60000 [==============================] - 173s 3ms/step - loss: 0.1598 - accuracy: 0.9523 - val_loss: 0.2465 - val_accuracy: 0.9575\n",
      "Epoch 2/6\n",
      "60000/60000 [==============================] - 172s 3ms/step - loss: 0.0572 - accuracy: 0.9828 - val_loss: 0.0432 - val_accuracy: 0.9863\n",
      "Epoch 3/6\n",
      "60000/60000 [==============================] - 171s 3ms/step - loss: 0.0361 - accuracy: 0.9891 - val_loss: 0.0377 - val_accuracy: 0.9873\n",
      "Epoch 4/6\n",
      "60000/60000 [==============================] - 173s 3ms/step - loss: 0.0263 - accuracy: 0.9919 - val_loss: 0.0420 - val_accuracy: 0.9868\n",
      "Epoch 5/6\n",
      "60000/60000 [==============================] - 177s 3ms/step - loss: 0.0184 - accuracy: 0.9947 - val_loss: 0.0377 - val_accuracy: 0.9880\n",
      "Epoch 6/6\n",
      "60000/60000 [==============================] - 184s 3ms/step - loss: 0.0141 - accuracy: 0.9955 - val_loss: 0.0355 - val_accuracy: 0.9884\n",
      "Batch Norm test accuracy: 0.9902999997138977\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 6\n",
    "model_bn = Sequential([\n",
    "    Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.25),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "model_bn.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "hist_bn = model_bn.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "bn_predict = model_bn.predict(x_test, verbose=0)\n",
    "score_bn = model_bn.evaluate(x_test, y_test, verbose=0)\n",
    "print('Batch Norm test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches train=469, test=79\n",
      "Epoch 1/6\n",
      "469/469 [==============================] - 109s 233ms/step - loss: 0.9914 - accuracy: 0.6801\n",
      "Epoch 2/6\n",
      "469/469 [==============================] - 111s 236ms/step - loss: 0.4083 - accuracy: 0.8773\n",
      "Epoch 3/6\n",
      "469/469 [==============================] - 111s 236ms/step - loss: 0.3481 - accuracy: 0.8953\n",
      "Epoch 4/6\n",
      "469/469 [==============================] - 111s 237ms/step - loss: 0.3155 - accuracy: 0.9046\n",
      "Epoch 5/6\n",
      "469/469 [==============================] - 112s 238ms/step - loss: 0.2962 - accuracy: 0.9107\n",
      "Epoch 6/6\n",
      "469/469 [==============================] - 114s 242ms/step - loss: 0.2818 - accuracy: 0.9140\n",
      "Image Norm Test Accuracy: 0.954\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 6\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "# prepare an iterators to scale images\n",
    "train_iterator = datagen.flow(x_train, y_train, batch_size=batch_size)\n",
    "test_iterator = datagen.flow(x_test, y_test, batch_size=batch_size)\n",
    "print('Batches train=%d, test=%d' % (len(train_iterator), len(test_iterator)))\n",
    "# confirm the scaling works\n",
    "batchX, batchy = train_iterator.next()\n",
    "\n",
    "model_in = Sequential([\n",
    "    Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    Dropout(0.25),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "# compile model\n",
    "model_in.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# fit model with generator\n",
    "model_in.fit_generator(train_iterator, steps_per_epoch=len(train_iterator), epochs=epochs)\n",
    "# evaluate model\n",
    "_, acc = model_in.evaluate_generator(test_iterator, steps=len(test_iterator), verbose=0)\n",
    "in_predict = model_in.predict(x_test, verbose=0)\n",
    "print(\"Image Norm Test Accuracy: {:.3f}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batches train=469, test=79\n",
      "Epoch 1/6\n",
      "469/469 [==============================] - 160s 340ms/step - loss: 0.1782 - accuracy: 0.9467\n",
      "Epoch 2/6\n",
      "469/469 [==============================] - 160s 342ms/step - loss: 0.0730 - accuracy: 0.9785\n",
      "Epoch 3/6\n",
      "469/469 [==============================] - 169s 360ms/step - loss: 0.0528 - accuracy: 0.9836\n",
      "Epoch 4/6\n",
      "469/469 [==============================] - 163s 349ms/step - loss: 0.0394 - accuracy: 0.9875\n",
      "Epoch 5/6\n",
      "469/469 [==============================] - 173s 368ms/step - loss: 0.0307 - accuracy: 0.9903\n",
      "Epoch 6/6\n",
      "469/469 [==============================] - 164s 350ms/step - loss: 0.0250 - accuracy: 0.9921\n",
      "Image + Batch Norm Test Accuracy: 0.889\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 6\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "# prepare an iterators to scale images\n",
    "train_iterator = datagen.flow(x_train, y_train, batch_size=batch_size)\n",
    "test_iterator = datagen.flow(x_test, y_test, batch_size=batch_size)\n",
    "print('Batches train=%d, test=%d' % (len(train_iterator), len(test_iterator)))\n",
    "# confirm the scaling works\n",
    "batchX, batchy = train_iterator.next()\n",
    "\n",
    "model_ibn = Sequential([\n",
    "    Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape),\n",
    "    BatchNormalization(),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.25),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "model_ibn.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_ibn.fit_generator(train_iterator, steps_per_epoch=len(train_iterator), epochs=epochs)\n",
    "# evaluate model\n",
    "_, acc = model_ibn.evaluate_generator(test_iterator, steps=len(test_iterator), verbose=0)\n",
    "ibn_predict = model_ibn.predict(x_test, verbose=0)\n",
    "print(\"Image + Batch Norm Test Accuracy: {:.3f}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/6\n",
      "60000/60000 [==============================] - 151s 3ms/step - loss: 0.2514 - accuracy: 0.9226 - val_loss: 0.0570 - val_accuracy: 0.9824\n",
      "Epoch 2/6\n",
      "60000/60000 [==============================] - 150s 3ms/step - loss: 0.0805 - accuracy: 0.9754 - val_loss: 0.0365 - val_accuracy: 0.9880\n",
      "Epoch 3/6\n",
      "60000/60000 [==============================] - 242s 4ms/step - loss: 0.0593 - accuracy: 0.9826 - val_loss: 0.0293 - val_accuracy: 0.9901\n",
      "Epoch 4/6\n",
      "60000/60000 [==============================] - 149s 2ms/step - loss: 0.0476 - accuracy: 0.9862 - val_loss: 0.0256 - val_accuracy: 0.9914\n",
      "Epoch 5/6\n",
      "60000/60000 [==============================] - 149s 2ms/step - loss: 0.0394 - accuracy: 0.9883 - val_loss: 0.0253 - val_accuracy: 0.9908\n",
      "Epoch 6/6\n",
      "60000/60000 [==============================] - 150s 3ms/step - loss: 0.0338 - accuracy: 0.9898 - val_loss: 0.0265 - val_accuracy: 0.9911\n",
      "Additional Conv2D Test accuracy:  0.991100013256073\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 6\n",
    "\n",
    "model_plus = Sequential([\n",
    "    Conv2D(32, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    Conv2D(32, (3, 3), activation='relu'),\n",
    "    Dropout(0.25),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "model_plus.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "hist = model_plus.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))\n",
    "plus_predict = model_plus.predict(x_test, verbose=0)\n",
    "score = model_plus.evaluate(x_test, y_test, verbose=0)\n",
    "print('Additional Conv2D Test accuracy: ', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "60000/60000 [==============================] - 145s 2ms/step - loss: 0.0593 - accuracy: 0.6944\n",
      "Epoch 2/6\n",
      "60000/60000 [==============================] - 141s 2ms/step - loss: 0.0931 - accuracy: 0.5342\n",
      "Epoch 3/6\n",
      "60000/60000 [==============================] - 157s 3ms/step - loss: 0.1271 - accuracy: 0.3644\n",
      "Epoch 4/6\n",
      "60000/60000 [==============================] - 142s 2ms/step - loss: 0.1474 - accuracy: 0.2631\n",
      "Epoch 5/6\n",
      "60000/60000 [==============================] - 142s 2ms/step - loss: 0.1223 - accuracy: 0.3884\n",
      "Epoch 6/6\n",
      "60000/60000 [==============================] - 138s 2ms/step - loss: 0.1585 - accuracy: 0.2076\n",
      "Epochs: 6, Optimizer: adam, Learning Rate: 0.01, Batch Size: 64\n",
      "Cutomized Model Test Accuracy:  0.10320000350475311\n",
      "Epoch 1/6\n",
      "60000/60000 [==============================] - 142s 2ms/step - loss: 0.0095 - accuracy: 0.9356\n",
      "Epoch 2/6\n",
      "60000/60000 [==============================] - 141s 2ms/step - loss: 0.0042 - accuracy: 0.9726\n",
      "Epoch 3/6\n",
      "60000/60000 [==============================] - 143s 2ms/step - loss: 0.0032 - accuracy: 0.9792\n",
      "Epoch 4/6\n",
      "60000/60000 [==============================] - 161s 3ms/step - loss: 0.0028 - accuracy: 0.9825\n",
      "Epoch 5/6\n",
      "60000/60000 [==============================] - 163s 3ms/step - loss: 0.0025 - accuracy: 0.9845\n",
      "Epoch 6/6\n",
      "60000/60000 [==============================] - 141s 2ms/step - loss: 0.0023 - accuracy: 0.9855\n",
      "Epochs: 6, Optimizer: adam, Learning Rate: 0.001, Batch Size: 64\n",
      "Cutomized Model Test Accuracy:  0.9894000291824341\n",
      "Epoch 1/6\n",
      "60000/60000 [==============================] - 135s 2ms/step - loss: 0.0093 - accuracy: 0.9375\n",
      "Epoch 2/6\n",
      "60000/60000 [==============================] - 133s 2ms/step - loss: 0.0043 - accuracy: 0.9726\n",
      "Epoch 3/6\n",
      "60000/60000 [==============================] - 132s 2ms/step - loss: 0.0034 - accuracy: 0.9789\n",
      "Epoch 4/6\n",
      "60000/60000 [==============================] - 133s 2ms/step - loss: 0.0028 - accuracy: 0.9829\n",
      "Epoch 5/6\n",
      "60000/60000 [==============================] - 133s 2ms/step - loss: 0.0025 - accuracy: 0.9845\n",
      "Epoch 6/6\n",
      "60000/60000 [==============================] - 131s 2ms/step - loss: 0.0022 - accuracy: 0.9862\n",
      "Epochs: 6, Optimizer: adagrad, Learning Rate: 0.01, Batch Size: 64\n",
      "Cutomized Model Test Accuracy:  0.9887999892234802\n",
      "Epoch 1/6\n",
      "60000/60000 [==============================] - 132s 2ms/step - loss: 0.0159 - accuracy: 0.8950\n",
      "Epoch 2/6\n",
      "60000/60000 [==============================] - 132s 2ms/step - loss: 0.0094 - accuracy: 0.9407\n",
      "Epoch 3/6\n",
      "60000/60000 [==============================] - 133s 2ms/step - loss: 0.0077 - accuracy: 0.9521\n",
      "Epoch 4/6\n",
      "60000/60000 [==============================] - 133s 2ms/step - loss: 0.0068 - accuracy: 0.9589\n",
      "Epoch 5/6\n",
      "60000/60000 [==============================] - 134s 2ms/step - loss: 0.0062 - accuracy: 0.9614\n",
      "Epoch 6/6\n",
      "60000/60000 [==============================] - 131s 2ms/step - loss: 0.0056 - accuracy: 0.9650\n",
      "Epochs: 6, Optimizer: adagrad, Learning Rate: 0.001, Batch Size: 64\n",
      "Cutomized Model Test Accuracy:  0.9768000245094299\n",
      "Epoch 1/6\n",
      "60000/60000 [==============================] - 114s 2ms/step - loss: 0.0160 - accuracy: 0.8890\n",
      "Epoch 2/6\n",
      "60000/60000 [==============================] - 113s 2ms/step - loss: 0.0094 - accuracy: 0.9418\n",
      "Epoch 3/6\n",
      "60000/60000 [==============================] - 112s 2ms/step - loss: 0.0093 - accuracy: 0.9449\n",
      "Epoch 4/6\n",
      "60000/60000 [==============================] - 113s 2ms/step - loss: 0.0168 - accuracy: 0.9121\n",
      "Epoch 5/6\n",
      "60000/60000 [==============================] - 111s 2ms/step - loss: 0.0324 - accuracy: 0.8368\n",
      "Epoch 6/6\n",
      "60000/60000 [==============================] - 111s 2ms/step - loss: 0.0317 - accuracy: 0.8413\n",
      "Epochs: 6, Optimizer: adam, Learning Rate: 0.01, Batch Size: 128\n",
      "Cutomized Model Test Accuracy:  0.9139000177383423\n",
      "Epoch 1/6\n",
      "60000/60000 [==============================] - 113s 2ms/step - loss: 0.0106 - accuracy: 0.9290\n",
      "Epoch 2/6\n",
      "60000/60000 [==============================] - 113s 2ms/step - loss: 0.0041 - accuracy: 0.9739\n",
      "Epoch 3/6\n",
      "60000/60000 [==============================] - 112s 2ms/step - loss: 0.0032 - accuracy: 0.9796\n",
      "Epoch 4/6\n",
      "60000/60000 [==============================] - 112s 2ms/step - loss: 0.0027 - accuracy: 0.9831\n",
      "Epoch 5/6\n",
      "60000/60000 [==============================] - 112s 2ms/step - loss: 0.0023 - accuracy: 0.9852\n",
      "Epoch 6/6\n",
      "60000/60000 [==============================] - 112s 2ms/step - loss: 0.0021 - accuracy: 0.9865\n",
      "Epochs: 6, Optimizer: adam, Learning Rate: 0.001, Batch Size: 128\n",
      "Cutomized Model Test Accuracy:  0.9891999959945679\n",
      "Epoch 1/6\n",
      "60000/60000 [==============================] - 106s 2ms/step - loss: 0.0101 - accuracy: 0.9326\n",
      "Epoch 2/6\n",
      "60000/60000 [==============================] - 107s 2ms/step - loss: 0.0040 - accuracy: 0.9744\n",
      "Epoch 3/6\n",
      "60000/60000 [==============================] - 107s 2ms/step - loss: 0.0030 - accuracy: 0.9809\n",
      "Epoch 4/6\n",
      "60000/60000 [==============================] - 107s 2ms/step - loss: 0.0025 - accuracy: 0.9844\n",
      "Epoch 5/6\n",
      "60000/60000 [==============================] - 105s 2ms/step - loss: 0.0022 - accuracy: 0.9861\n",
      "Epoch 6/6\n",
      "60000/60000 [==============================] - 105s 2ms/step - loss: 0.0020 - accuracy: 0.9874\n",
      "Epochs: 6, Optimizer: adagrad, Learning Rate: 0.01, Batch Size: 128\n",
      "Cutomized Model Test Accuracy:  0.9879999756813049\n",
      "Epoch 1/6\n",
      "60000/60000 [==============================] - 107s 2ms/step - loss: 0.0177 - accuracy: 0.8830\n",
      "Epoch 2/6\n",
      "60000/60000 [==============================] - 107s 2ms/step - loss: 0.0104 - accuracy: 0.9341\n",
      "Epoch 3/6\n",
      "60000/60000 [==============================] - 106s 2ms/step - loss: 0.0085 - accuracy: 0.9459\n",
      "Epoch 4/6\n",
      "60000/60000 [==============================] - 107s 2ms/step - loss: 0.0074 - accuracy: 0.9530\n",
      "Epoch 5/6\n",
      "60000/60000 [==============================] - 107s 2ms/step - loss: 0.0067 - accuracy: 0.9586\n",
      "Epoch 6/6\n",
      "60000/60000 [==============================] - 107s 2ms/step - loss: 0.0062 - accuracy: 0.9623\n",
      "Epochs: 6, Optimizer: adagrad, Learning Rate: 0.001, Batch Size: 128\n",
      "Cutomized Model Test Accuracy:  0.9735999703407288\n",
      "Best Accuracy : 0.9894000291824341\n"
     ]
    }
   ],
   "source": [
    "batch_size_list = [64,128]\n",
    "epochs = 6\n",
    "optimizers_list=[\"adam\",\"adagrad\"]\n",
    "learning_rates = [.01, .001]\n",
    "best_predict = None\n",
    "best_acc = 0\n",
    "for batch_size in batch_size_list:\n",
    "    for optimizer_str in optimizers_list:\n",
    "        for learning_rate in learning_rates:\n",
    "            model_custom = Sequential([\n",
    "                Conv2D(32, kernel_size=(3, 3),\n",
    "                             activation='relu',\n",
    "                             input_shape=input_shape),\n",
    "                Conv2D(64, (3, 3), activation='relu'),\n",
    "                Dropout(0.25),\n",
    "                Flatten(),\n",
    "                Dense(128, activation='relu'),\n",
    "                Dropout(0.5),\n",
    "                Dense(num_classes, activation='softmax')\n",
    "            ])\n",
    "            optimizer = None\n",
    "            if optimizer_str == \"adam\":\n",
    "                optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "            else:\n",
    "                optimizer = keras.optimizers.Adagrad(learning_rate=learning_rate)\n",
    "\n",
    "            model_custom.compile(optimizer=optimizer,\n",
    "                          loss='mse',\n",
    "                          metrics=[\"accuracy\"])\n",
    "\n",
    "            hist = model_custom.fit(x_train, y_train, epochs=epochs, batch_size = batch_size)\n",
    "            score = model_custom.evaluate(x_test, y_test, verbose=0)\n",
    "            if score[1] > best_acc:\n",
    "                best_acc = score[1]\n",
    "                best_predict = model_custom.predict(x_test, verbose=0)\n",
    "            print(\"Epochs: {}, Optimizer: {}, Learning Rate: {}, Batch Size: {}\".format(str(epochs), optimizer_str, str(learning_rate), str(batch_size)))\n",
    "            print('Cutomized Model Test Accuracy: ', score[1])\n",
    "print(\"Best Accuracy : {}\".format(best_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Optimizer  | Learning Rate  | Batch Size  | Test Accuracy  |\n",
    "|---|---|---|---|\n",
    "| Adam  | .01  | 64  | .1032  |\n",
    "|  Adam  | .001  | 64  | .9894  |\n",
    "|  Adagrad  | .01  | 64  | .9888  |\n",
    "|  Adagrad  | .001  | 64   | .9768  |\n",
    "| Adam  | .01  | 128  | .9139  |\n",
    "|  Adam  | .001  | 128  | .9892  |\n",
    "|  Adagrad  | .01  | 128  | .9880  |\n",
    "|  Adagrad  | .001  | 128   |  .9736 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 977    0    0    0    0    0    1    1    1    0]\n",
      " [   0 1130    2    0    0    1    2    0    0    0]\n",
      " [   1    3 1022    0    1    0    0    3    2    0]\n",
      " [   0    0    6 1000    0    1    0    2    1    0]\n",
      " [   1    0    1    0  973    0    1    0    2    4]\n",
      " [   2    0    0    3    0  878    6    0    0    3]\n",
      " [   3    2    0    1    1    2  948    0    1    0]\n",
      " [   1    0    9    0    0    0    0 1012    1    5]\n",
      " [   4    0    1    0    0    1    0    1  964    3]\n",
      " [   3    0    0    0    4    3    0    2    7  990]]\n"
     ]
    }
   ],
   "source": [
    "y_predict_amax = np.array([np.argmax(y) for y in best_predict])\n",
    "y_test_amax = np.array([np.argmax(y) for y in y_test])\n",
    "matrix = confusion_matrix(y_test_amax,y_predict_amax)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "According to the above table with the customized hyperparameters, the best model was the one with the Adam optimizer with a learning rate of .001 with a batch size of 64. The worst performing one, by a large margin, was the Adam with a .01 learning rate and a batch size of 64.\n",
    "\n",
    "Based on the results of everything else, batch normalization or image normalization by themselves (or neither) boasted high testing accuracy.On the other hand, strangely, combining batch and image normalization diminished the overall performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
